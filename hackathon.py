# -*- coding: utf-8 -*-
"""Hackathon.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UD-W5mRaqH2RpuBxdlYmSJfkQQ6zaP-0
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# !pip install Transformers

from sklearn.model_selection import train_test_split
from transformers import TFAutoModel, AutoTokenizer, BertForSequenceClassification, pipeline
from sklearn.preprocessing import LabelEncoder

# !pip install Torch

import torch

df=pd.read_csv('/content/DatasetCopy.csv')
df

df.drop("page_id", inplace=True, axis=1)

df = df.dropna()
df = df.reset_index(drop=True)

df

df.isnull().sum()

df.groupby('Pattern Category').describe()

fine = df['Pattern Category'].unique().tolist()
fine = [s.strip() for s in fine]
fine

num_fine = len(fine)
id2fine = {id:fine for id, fine in enumerate(fine)}
fine2id = {fine:id for id, fine in enumerate(fine)}

df.drop('label', axis=1, inplace=True)
df.head()

plt.figure(figsize=(8, 6))
sns.countplot(x='Pattern Category', data=df)
plt.title('Class Distribution')
plt.show()

plt.figure(figsize=(8, 8))
df['Pattern Category'].value_counts().plot.pie(autopct='%1.1f%%', startangle=90)
plt.title('Pattern Category Distribution')
plt.show()

df['labels'] = df['Pattern Category'].map(lambda x: fine2id[x.strip()])
df

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased", max_length=512)

model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=num_fine,
                                                      id2label=id2fine, label2id=fine2id)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model.to(device)

SIZE = df.shape[0]
train_texts = list(df.text[:SIZE//2])
val_texts = list(df.text[SIZE//2: (SIZE//2) + (SIZE//4)])
test_texts = list(df.text[(3*SIZE)//4:])
train_labels = list(df.labels[:SIZE//2])
val_labels = list(df.labels[SIZE//2: (SIZE//2) + (SIZE//4)])
test_labels = list(df.labels[(3*SIZE)//4:])

len(train_texts), len(val_texts), len(test_texts)

train_encodings = tokenizer(train_texts, truncation=True, padding=True)
if len(val_texts) > 0:
    val_encodings = tokenizer(val_texts, truncation=True, padding=True)
else:
    val_encodings = None  # or handle it accordingly)
test_encodings = tokenizer(test_texts, truncation=True, padding=True)

from torch.utils.data import Dataset

class Dataloader(Dataset):
    def __init__(self, encodings, labels):

        print("Encodings:", encodings)
        print("Labels:", labels)
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataloader = Dataloader(train_encodings, train_labels)
val_dataloader = Dataloader(val_encodings, val_labels)
test_dataloader = Dataloader(test_encodings, test_labels)

from transformers import TrainingArguments, Trainer

from sklearn.metrics import accuracy_score, precision_recall_fscore_support

def compute_metrics(pred):
    labels = pred.label_ids

    # Obtain predicted class labels by finding the column index with the maximum probability
    preds = pred.predictions.argmax(-1)

    # Compute macro precision, recall, and F1 score using sklearn's precision_recall_fscore_support function
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')

    # Calculate the accuracy score using sklearn's accuracy_score function
    acc = accuracy_score(labels, preds)

    # Return the computed metrics as a dictionary
    return {
        'Accuracy': acc,
        'F1': f1,
        'Precision': precision,
        'Recall': recall
    }

# !pip install accelerate

# !pip install accelerate>=0.20.1

training_args = TrainingArguments(
    output_dir="./TTC4908Model",
    do_train=True,
    do_eval=True,

    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,

    warmup_steps=100,
    weight_decay=0.01,
    logging_strategy="steps",

    logging_dir="./multi-class-logs",
    logging_steps=50,
    evaluation_strategy="steps",
    eval_steps=50,
    save_strategy="steps",
    fp16=False,
    load_best_model_at_end=True,
)

trainer = Trainer(
    # the pre-trained model that will be fine-tuned
    model=model,

    # training arguments that we defined above
    args=training_args,
    train_dataset=train_dataloader,
    eval_dataset=val_dataloader,
    compute_metrics=compute_metrics
)

# !pip install transformers==4.12.0 accelerate==0.20.1



trainer.train()

def predict(text):
    inputs = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors="pt").to("cuda")

    # Get model output (logits)
    outputs = model(**inputs)

    probs = outputs['logits'].softmax(1)
    pred_label_idx = probs.argmax()

    # Now map the predicted class index to the actual class label

    # Since pred_label_idx is a tensor containing a single value (the predicted class index),
    # the .item() method is used to extract the value as a scalar

    pred_label = model.config.id2label[pred_label_idx.item()]

    return probs, pred_label_idx, pred_label

model_path = "DPBH_BERT_Fine_Tuned_Model"
trainer.save_model(model_path)
tokenizer.save_pretrained(model_path)

from transformers import BertTokenizerFast

model_path = "DPBH_BERT_Fine_Tuned_Model"

model = BertForSequenceClassification.from_pretrained(model_path)
tokenizer = BertTokenizerFast.from_pretrained(model_path)
nlp = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)

nlp("42 sold in last 24 hours")

# !pip install selenium

# !pip install requests

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service







"""chrome_options = Options()
chrome_options.add_argument("--headless")  # Use headless mode if you don't want a visible browser window

service = Service(executable_path="/content/chromedriver.exe")
options = webdriver.ChromeOptions()
options.add_argument("--headless")  # Uncomment this line if you want headless mode

driver = webdriver.Chrome(service=service, options=options)


#driver = webdriver.Chrome(service=service)
"""

# service = Service("C:\\Users\\HP\\Downloads\\chromedriver_win32>")
# options = webdriver.ChromeOptions()
# driver = webdriver.Chrome(service=service, options=options)















import requests
from bs4 import BeautifulSoup

headers={'User-Agent':'Mozilla/5.0 (Windows NT 6.3; Win 64 ; x64) Apple WeKit /537.36(KHTML , like Gecko) Chrome/80.0.3987.162 Safari/537.36'}
webpage=requests.get('https://www.ebay.com/itm/126112806945?_trkparms=amclksrc%3DITM%26aid%3D777008%26algo%3DPERSONAL.TOPIC%26ao%3D1%26asc%3D20230823115209%26meid%3D0b54a331b3ef46a1be586391e8e01800%26pid%3D101800%26rk%3D1%26rkt%3D1%26sd%3D126112806945%26itm%3D126112806945%26pmt%3D0%26noa%3D1%26pg%3D4375194%26algv%3DRecentlyViewedItemsV2SignedOut%26brand%3DLenovo&_trksid=p4375194.c101800.m5481&_trkparms=parentrq%3A566200ba18d0a8d3a3b9275bfffc4882%7Cpageci%3A67ce43b8-bed0-11ee-8ed5-223cb2e4e976%7Ciid%3A1%7Cvlpname%3Avlp_homepage',headers=headers).text

soup=BeautifulSoup(webpage,'lxml')

print(soup.prettify())

for i in soup.find_all('span',class_='ux-textspans ux-textspans--BOLD ux-textspans--EMPHASIS'):
  print(i.text.strip())

